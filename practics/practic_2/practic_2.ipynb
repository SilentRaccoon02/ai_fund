{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering with Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорты\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../../config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from category_encoders import MEstimateEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общая информация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(cfg[\"house_pricing\"][\"train_dataset\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(cfg[\"house_pricing\"][\"test_dataset\"])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не все столбцы здесь выведены. Их список мы можем получить, используя аттрибут `columns`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почистим данные в нескольких столбцах, основываясь на data_description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Exterior2nd\"] = train_df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n",
    "# Some values of GarageYrBlt are corrupt, so we'll replace them\n",
    "# with the year the house was built\n",
    "train_df[\"GarageYrBlt\"] = train_df[\"GarageYrBlt\"].where(\n",
    "    train_df.GarageYrBlt <= 2010, train_df.YearBuilt\n",
    ")\n",
    "# Names beginning with numbers are awkward to work with\n",
    "train_df.rename(\n",
    "    columns={\n",
    "        \"1stFlrSF\": \"FirstFlrSF\",\n",
    "        \"2ndFlrSF\": \"SecondFlrSF\",\n",
    "        \"3SsnPorch\": \"Threeseasonporch\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = train_df.select_dtypes(include=[\"object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = train_df.select_dtypes(exclude=[\"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коррелирующие признаки\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 5, figsize=(20, 20))\n",
    "axes_flattened = axes.reshape(-1)\n",
    "for i in range(len(num_df.columns)):\n",
    "    ax = axes_flattened[i]\n",
    "    sns.scatterplot(\n",
    "        x=num_df.iloc[:, i],\n",
    "        y=\"SalePrice\",\n",
    "        data=num_df.dropna(),\n",
    "        ax=ax,\n",
    "    )\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n",
    "    sns.clustermap(\n",
    "        df.corr(method),\n",
    "        vmin=-1.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"icefire\",\n",
    "        method=\"complete\",\n",
    "        annot=annot,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "corrplot(num_df, annot=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(\n",
    "        df.corr(method),\n",
    "        vmin=-1.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"icefire\",\n",
    "        annot=annot,\n",
    "        ax=ax,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "corrplot(num_df, annot=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из этой матрицы можно увидеть, какие столбцы сильно коррелируют между собой, например:\n",
    "\n",
    "1. GarageYrBlt и YearBuilt\n",
    "2. TotRmsAbvGrd и GrLivArea\n",
    "3. FirstFlrSF и TotalBsmtSF\n",
    "4. GarageArea и GarageCars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Выведите 5 пар наиболее сильно коррелирующих признаков. Сколько пар признаков коррелирует больше, чем на 0.75?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = num_df.corr().abs()\n",
    "np.fill_diagonal(m.values, 0)\n",
    "pairs = m.stack().reset_index()\n",
    "pairs.columns = [\"1\", \"2\", \"corr\"]\n",
    "pairs = pairs.sort_values(by=\"corr\", ascending=False)\n",
    "print(pairs.head(5))\n",
    "print((pairs[\"corr\"] > 0.75).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Постройте карту корреляций, но не используя clustermap. Используйте для этого heatmap. Ответьте на вопрос, те же пары признаков наиболее ярко подсвечены или разные? По какому из графиков вам удобнее делать выводы?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(\n",
    "    num_df.corr(\"pearson\"), vmin=-1.0, vmax=1.0, cmap=\"icefire\", annot=None, ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Выбросьте несколько лишних признаков из датасета.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(\n",
    "    [\"GarageYrBlt\", \"TotRmsAbvGrd\", \"FirstFlrSF\", \"GarageCars\"], axis=1, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заполнение пустых значений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может быть множество вариантов, при которых строка может содержать пустые значения. Например:\n",
    "\n",
    "1. Дом с 2 спальнями не может включать ответ на вопрос, насколько велика третья спальня\n",
    "2. Кто-то из опрошенных может не делиться своим доходом\n",
    "3. ...\n",
    "\n",
    "Библиотеки Python представляют недостающие числа как NaN-ми, что является сокращением от \"not a number\".\n",
    "\n",
    "Соберем статистику, связанную с NaN-ми. Какие ячейки имеют недостающие значения (в процентах), можно определить с помощью команды:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df = (train_df.isnull().mean() * 100).reset_index()\n",
    "nan_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем этот датафрейм в более изящный вид:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df.columns = [\"column_name\", \"percentage\"]\n",
    "nan_df.sort_values(\"percentage\", ascending=False, inplace=True)\n",
    "nan_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем квантили:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_srt = \"Number of columns with more than\"\n",
    "for percent in (80, 50, 20, 5):\n",
    "    print(f\"{intro_srt} {percent}% NANs: {(nan_df.percentage > percent).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем столбцы с более чем 80% NaN-в\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_columns = list(nan_df[nan_df.percentage > 80][\"column_name\"])\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство моделей не умеют работать с NaN-ми. Поэтому требуется избавиться от них.\n",
    "Вообще есть три вида отсутствия значений: Полностью случайное, случайное и неслучайное.\n",
    "\n",
    "-   Полностью случайное не связано ни с какими другими зависимостями в датасете. Удаление таких данных никак не повлияет на результаты.\n",
    "-   Случайное связано с наблюдаемыми переменными, но не с ненаблюдаемыми. Это самый широкий класс, и с ним борется большинство методов.\n",
    "-   Случайное, которое связано с ненаблюдаемыми переменными (например, старением оборудования).\n",
    "    В зависимости от того, с каким видом пропусков мы имеем дело, можно использовать разные методы.\n",
    "\n",
    "### Выброс стоблцов с NaN-ми\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 вариант - если, например, нужно выбросить одинаковые\n",
    "# столбцы для обучающей и тестовой выборок\n",
    "num_сols_with_missing = [col for col in num_df.columns if num_df[col].isnull().any()]\n",
    "num_сols_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(num_df.columns))\n",
    "num_df_dropped = num_df.drop(num_сols_with_missing, axis=1)\n",
    "print(len(num_df_dropped.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 опция:** выбросить столбцы, напрямую используя `dropna()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(num_df.columns))\n",
    "num_df_dropped = num_df.dropna(axis=1)\n",
    "print(len(num_df_dropped.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если эти столбцы содержат полезную информацию (в местах, которые не были пропущены), модель теряет доступ к этой информации при удалении столбца. Кроме того, если тестовые данные имеют отсутствующие значения в тех местах, где тренировочные не имели, это приведет к ошибке.\n",
    "Однако оно может быть полезно, когда большинство значений в столбце отсутствуют.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: выбросьте столбцы, у которых отсутствует больше 75% значений. Сделайе это и для тренировочных, и для тестовых данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(data):\n",
    "    missing_percentage = data.isnull().mean()\n",
    "    columns_to_drop = missing_percentage[missing_percentage > 0.75].index\n",
    "    data.drop(columns=columns_to_drop, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_df = remove_columns(train_df)\n",
    "test_df = remove_columns(test_df)\n",
    "\n",
    "print(\"train\", train_df.shape)\n",
    "print(\"test\", test_df.shape)\n",
    "\n",
    "nan_percentage_train = train_df.isnull().mean() * 100\n",
    "nan_percentage_test = test_df.isnull().mean() * 100\n",
    "\n",
    "for percent in (75, 50, 20, 5):\n",
    "    print(f\"{percent}% train nan {(nan_percentage_train > percent).sum()}\")\n",
    "    print(f\"{percent}% test nan {(nan_percentage_test > percent).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполнение недостающих значений каким-то значением\n",
    "\n",
    "Это значение будет не совсем правильным в большинстве случаев, но обычно оно дает более точные модели, чем полное удаление столбца.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Числовые признаки**\n",
    "\n",
    "Поведение по умолчанию заполняет столбец средним значением в заполненных ячейках. Существуют и более сложные стратегии.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# --- restore values ---\n",
    "train_df = pd.read_csv(cfg[\"house_pricing\"][\"train_dataset\"])\n",
    "nan_df = (train_df.isnull().mean() * 100).reset_index()\n",
    "nan_df.columns = [\"column_name\", \"percentage\"]\n",
    "nan_df.sort_values(\"percentage\", ascending=False, inplace=True)\n",
    "nan_columns = list(nan_df[nan_df.percentage > 80][\"column_name\"])\n",
    "print(nan_columns)\n",
    "# --- restore values ---\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "filled_cols = my_imputer.fit_transform(train_df[num_сols_with_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативно можно заполнить столбцы средним напрямую (или нулями, или чем угодно)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения статистики такое заполнение оправдано, если все между признаками нет явной зависимости. В таком случае замена пропусков средними значениями не вносит смещения. Однако, часто условие независимости нарушается.\n",
    "\n",
    "Взглянем на распределения средних значений по районам.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_grouped = train_df.groupby(\"Neighborhood\")\n",
    "neigh_lot = neigh_grouped[\"LotFrontage\"].mean().reset_index(name=\"LotFrontage_mean\")\n",
    "neigh_garage = neigh_grouped[\"GarageArea\"].mean().reset_index(name=\"GarageArea_mean\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 8))\n",
    "axes[0].tick_params(axis=\"x\", rotation=90)\n",
    "sns.barplot(x=\"Neighborhood\", y=\"LotFrontage_mean\", data=neigh_lot, ax=axes[0])\n",
    "axes[1].tick_params(axis=\"x\", rotation=90)\n",
    "sns.barplot(x=\"Neighborhood\", y=\"GarageArea_mean\", data=neigh_garage, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном примере свойства домов сильно зависят от того, в каком районе они расположены. Поэтому средние значения лучше считать по районам. Это будет простой вариант построения модели для заполнения значений. Вы также можете строить линейные модели или любые другие.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"LotFrontage\"] = train_df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "train_df[\"GarageArea\"] = train_df.groupby(\"Neighborhood\")[\"GarageArea\"].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним все оставшиеся числовые признаки средними (ранее мы не сохраняли результат в `train_df`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[num_сols_with_missing] = train_df[num_сols_with_missing].fillna(\n",
    "    train_df[num_сols_with_missing].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Те столбцы, которые содержали более 80% NANов, удалим совсем\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(nan_columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Категориальные (номинальные) признаки**\n",
    "\n",
    "Для заполнения категориапльных признаков естественно использовать значение по умолчанию \"NA\" или \"missing\".\n",
    "\n",
    "Понятие среднего здесь тяжело использовать, поэтому проще заполнить модой, то есть наиболее часто встречающимся значением.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"MasVnrType\",\n",
    "    \"MSZoning\",\n",
    "    \"Exterior1st\",\n",
    "    \"Exterior2nd\",\n",
    "    \"SaleType\",\n",
    "    \"Electrical\",\n",
    "    \"Functional\",\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "    print(f\"Mode of column {col} is {train_df[col].dropna().mode()[0]}\")\n",
    "\n",
    "\n",
    "def fillna(x):\n",
    "    mode_value = x.dropna().mode()\n",
    "    if not mode_value.empty:\n",
    "        return x.fillna(mode_value[0])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "train_df[cols] = train_df.groupby(\"Neighborhood\")[cols].transform(fillna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Порядковые признаки**\n",
    "\n",
    "Значение \"NA\" удобно ассоциировать с нулем. Но такая замена меняет число уровней у признака, что может быть нежелательно. Более того, часто неочевидно, какой уровень признака соответствует NA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = [\n",
    "    \"GarageType\",\n",
    "    \"GarageFinish\",\n",
    "    \"BsmtFinType2\",\n",
    "    \"BsmtExposure\",\n",
    "    \"BsmtFinType1\",\n",
    "    \"GarageCond\",\n",
    "    \"GarageQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"BsmtQual\",\n",
    "    \"FireplaceQu\",\n",
    "    \"KitchenQual\",\n",
    "    \"HeatingQC\",\n",
    "    \"ExterQual\",\n",
    "    \"ExterCond\",\n",
    "]\n",
    "train_df[cat] = train_df[cat].fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из основных недостатков простого заполнения является то, что оно может привести к невозможным конфигурациям в данных. Представьте, что гараж отсутствует, но мы знаем число мест для машин. И хотя это неверно с точки зрения логики, это может быть все равно улучшением относительно ситуации, когда модель вообще не работала на примере с пропуском.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Используя simple imputer или заполнение модой на исходных тренировочных данных (вероятно, копии), создайте и найдите невозможную комбинацию (комбинации) в данных. Покажите пример(ы).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "train_df_copy = train_df.copy()\n",
    "print(train_df_copy.shape)\n",
    "\n",
    "num_cols = train_df_copy.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "train_df_copy[num_cols] = imputer.fit_transform(train_df_copy[num_cols])\n",
    "\n",
    "cat_cols = train_df_copy.select_dtypes(include=[\"object\"]).columns\n",
    "train_df_copy[cat_cols] = train_df_copy[cat_cols].fillna(\"NA\")\n",
    "\n",
    "c_1 = pd.DataFrame(\n",
    "    {\n",
    "        \"GarageArea\": [0],\n",
    "        \"GarageCars\": [2],\n",
    "        \"GarageType\": [\"NA\"],\n",
    "        \"SalePrice\": [200000],\n",
    "    }\n",
    ")\n",
    "\n",
    "train_df_copy = pd.concat([train_df_copy, c_1], ignore_index=True)\n",
    "print(train_df_copy.shape)\n",
    "\n",
    "c_1 = train_df_copy[\n",
    "    (train_df_copy[\"GarageArea\"] == 0) & (train_df_copy[\"GarageCars\"] > 0)\n",
    "]\n",
    "\n",
    "print(c_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другой минус константного заполнения - уменьшение дисперсии в данных, изменение распределения. Это происходит, так как мы добавляем много примеров с нулевой дисперсией. В том числе и поэтому полезно смотреть на зависмимость признака от других переменных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Возьмите любой числовой столбец с достаточно хорошим распределением (в идеале, чтобы у гистограммы было несколько \"горбов\". Можете сгенерировать или взять из данных. Выбросьте из него 10, 20, 50 и 70% примеров. Покажите на графиках, как меняется гистограмма.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.random.normal(loc=40, scale=8, size=800)\n",
    "data2 = np.random.normal(loc=80, scale=16, size=1600)\n",
    "data3 = np.random.normal(loc=120, scale=8, size=400)\n",
    "\n",
    "data = np.concatenate([data1, data2, data3])\n",
    "\n",
    "\n",
    "def remove_data(data, percent):\n",
    "    n = int(len(data) * percent / 100)\n",
    "    i_remove = np.random.choice(len(data), n, replace=False)\n",
    "    return np.delete(data, i_remove)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(data, bins=50, kde=True)\n",
    "plt.title(\"100\")\n",
    "\n",
    "data_20 = remove_data(data, 20)\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(data_20, bins=50, kde=True)\n",
    "plt.title(\"-20%\")\n",
    "\n",
    "data_50 = remove_data(data, 50)\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(data_50, bins=50, kde=True)\n",
    "plt.title(\"-50%\")\n",
    "\n",
    "data_70 = remove_data(data, 70)\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(data_70, bins=50, kde=True)\n",
    "plt.title(\"-70%\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Попробуйте найти зависимость какой-нибудь категориальной переменной от других (от одного столбца). Заполните ее с учетом этой зависимости.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- restore values ---\n",
    "train_df = pd.read_csv(cfg[\"house_pricing\"][\"train_dataset\"])\n",
    "# --- restore values ---\n",
    "\n",
    "train_df_copy = train_df.copy()\n",
    "missing_before = train_df_copy[\"Fence\"].isnull().sum()\n",
    "garage_price_avg = train_df_copy.groupby(\"Fence\")[\"SalePrice\"].mean()\n",
    "train_df_copy[\"Fence\"] = train_df_copy[\"Fence\"].fillna(train_df_copy[\"Fence\"].mode()[0])\n",
    "missing_after = train_df_copy[\"Fence\"].isnull().sum()\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "sns.stripplot(\n",
    "    data=train_df_copy,\n",
    "    x=\"Fence\",\n",
    "    y=\"SalePrice\",\n",
    "    palette=\"viridis\",\n",
    "    hue=\"Fence\",\n",
    ")\n",
    "plt.title(\"SalePrice/Fence\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Fence\", fontsize=14)\n",
    "plt.ylabel(\"SalePrice\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"missing_before {missing_before}\")\n",
    "print(f\"missing_after {missing_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление признаков со слабой вариативностью\n",
    "\n",
    "**Признаки с одним типичным значением**\n",
    "\n",
    "Некоторые признаки в основном состоят из одного значения или нулей, что не особо полезно для нас. Поэтому мы устанавливаем пороговое значение, определяемое пользователем, на уровне 96%. Если столбец имеет более 96% от одного и того же значения, мы считаем признак бесполезными и удалим его.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_almost_constant_columns(df, dropna=True):\n",
    "    cols = []\n",
    "    for i in df:\n",
    "        if dropna:\n",
    "            counts = df[i].dropna().value_counts()\n",
    "        else:\n",
    "            counts = df[i].value_counts()\n",
    "        most_popular_value_count = counts.iloc[0]\n",
    "        if (most_popular_value_count / len(df)) * 100 > 96:\n",
    "            cols.append(i)\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = train_df.select_dtypes(include=[\"object\"])\n",
    "overfit_cat = get_almost_constant_columns(cat_df)\n",
    "train_df = train_df.drop(overfit_cat, axis=1)\n",
    "overfit_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = train_df.select_dtypes(exclude=[\"object\"])\n",
    "overfit_num = get_almost_constant_columns(num_df, dropna=True)\n",
    "train_df = train_df.drop(overfit_num, axis=1)\n",
    "overfit_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда слабая вариативность все же не означает, что признак бесполезен. Это возможно для очень сильно несбалансированной переменной с малым количеством уровней. Более того, иногда такая несбалансированность возникает и в таргете!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Признаки с маленькой дисперсией**\n",
    "\n",
    "Другой способ - использовать метод VarianceThreshold от sklearn — это простой базовый подход к выбору признаков. Он удаляет все признаки, дисперсия которых не соответствует определенному порогу. По умолчанию он удаляет все элементы с нулевой дисперсией, т.е. те элементы, которые имеют одинаковое значение у всех семплов.\n",
    "\n",
    "Стоит отметить, что дисперсия является абсолютной величиной, и выбор порога в этом случае является эмпирическим. При этом в общем случае малые значения дисперсии не говорят о бесполезности признака. Если признак задан на поле вещественных чисел, то его дискриминирующая способность не зависит от дисперсии, так как любой непрерывный интервал на вещественной оси содержит бесконечный набор значений. Однако, в случае дискретных значений (пример, целочисленных признаков) VarianceThreshold действительно становится полезным\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sel = VarianceThreshold(threshold=0.1)\n",
    "num_col = train_df.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "sel.fit(num_col)  # fit finds the features with low variance\n",
    "sum(sel.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `get_support()` возвратит булевскую маску для признаков, которые проходят указанный порог по дисперсии. Ее можно использовать для отбора этих признаков\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, таким образом мы получаем список всех признаков, которые были отсеяны данным алгоритмом:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col.columns[~sel.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление выбросов\n",
    "\n",
    "Удаление выбросов предотвратит воздействие экстремальных значений на производительность наших моделей.\n",
    "\n",
    "Из скаттерплотов выше мы можем увидеть, что следующие признаки имеют экстремальные выбросы:\n",
    "\n",
    "-   LotFrontage\n",
    "-   LotArea\n",
    "-   BsmtFinSF1\n",
    "-   TotalBsmtSF\n",
    "-   GrLivArea\n",
    "\n",
    "Мы уберем выбросы на основе определенного порогового значения.\n",
    "Эти значения мы получим из боксплотов (\"ящик с усиками\"):\n",
    "\n",
    "![Boxplot](boxplot.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_col = [\"LotFrontage\", \"LotArea\", \"BsmtFinSF1\", \"TotalBsmtSF\", \"GrLivArea\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for ax, col in zip(axes, out_col):\n",
    "    sns.boxplot(y=train_df[col], data=train_df, ax=ax)\n",
    "fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, upper_bound in (\n",
    "    (\"LotFrontage\", 200),\n",
    "    (\"LotArea\", 100000),\n",
    "    (\"BsmtFinSF1\", 4000),\n",
    "    (\"TotalBsmtSF\", 5000),\n",
    "    (\"GrLivArea\", 4000),\n",
    "):\n",
    "    train_df = train_df.drop(train_df[train_df[col] > upper_bound].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После удаления выбросов, сильно коррелированных признаков и условных отсутствующих значений мы можем приступить к добавлению дополнительной информации для обучения нашей модели. Это делается с помощью - Feature Engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Создайте копию данных. Удалите из числовых столбцов выбросы (используя интерквартильный размах)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_copy = train_df.copy()\n",
    "print(train_df_copy.shape)\n",
    "\n",
    "cols = train_df_copy.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "\n",
    "for col in cols:\n",
    "    Q1 = train_df_copy[col].quantile(0.25)\n",
    "    Q3 = train_df_copy[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    train_df_copy = train_df_copy[\n",
    "        (train_df_copy[col] >= lower_bound) & (train_df_copy[col] <= upper_bound)\n",
    "    ]\n",
    "\n",
    "print(train_df_copy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Определите, есть ли выбросы в LotArea после группировки по районам. Есть ли районы, в которых выбросы сильнее, чем в остальных? Есть ли связь признака \"наличие выброса\" c качеством дома?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_group = train_df.groupby(\"Neighborhood\")[\"LotArea\"]\n",
    "\n",
    "\n",
    "def out(group):\n",
    "    Q1 = group.quantile(0.25)\n",
    "    Q3 = group.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return group[(group < lower_bound) | (group > upper_bound)]\n",
    "\n",
    "\n",
    "out_neigh = neigh_group.apply(out)\n",
    "out_count = out_neigh.groupby(\"Neighborhood\").count()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=out_count.index, y=out_count.values)\n",
    "plt.title(\"dist/out\")\n",
    "plt.xlabel(\"dist\")\n",
    "plt.ylabel(\"out\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "train_df[\"has_outlier\"] = train_df[\"LotArea\"].apply(\n",
    "    lambda x: 1 if x in out_neigh.values else 0\n",
    ")\n",
    "\n",
    "corr = train_df[[\"has_outlier\", \"OverallQual\"]].corr()\n",
    "print(f\"correlation {corr.loc['has_outlier', 'OverallQual']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - это техника, с помощью которой мы создаем новые признаки, которые потенциально могут помочь в прогнозировании нашей целевой переменной, которая в данном случае является SalePrice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSSubClass - это столбец с числовым признаком, который на самом деле можно представить как категориальный\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"MSSubClass\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"MSSubClass\"] = train_df[\"MSSubClass\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_map = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0}\n",
    "fintype_map = {\"GLQ\": 6, \"ALQ\": 5, \"BLQ\": 4, \"Rec\": 3, \"LwQ\": 2, \"Unf\": 1, \"NA\": 0}\n",
    "expose_map = {\"Gd\": 4, \"Av\": 3, \"Mn\": 2, \"No\": 1, \"NA\": 0}\n",
    "fence_map = {\"GdPrv\": 4, \"MnPrv\": 3, \"GdWo\": 2, \"MnWw\": 1, \"NA\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_col = [\n",
    "    \"ExterQual\",\n",
    "    \"ExterCond\",\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"HeatingQC\",\n",
    "    \"KitchenQual\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "    \"FireplaceQu\",\n",
    "]\n",
    "for col in ord_col:\n",
    "    train_df[col] = train_df[col].map(ordinal_map)\n",
    "\n",
    "fin_col = [\"BsmtFinType1\", \"BsmtFinType2\"]\n",
    "for col in fin_col:\n",
    "    train_df[col] = train_df[col].map(fintype_map)\n",
    "\n",
    "train_df[\"BsmtExposure\"] = train_df[\"BsmtExposure\"].map(expose_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основываясь на текущих признаках, мы можем добавить первый дополнительный признак, который будет называться TotalLot и который суммирует LotFrontage и LotArea для определения общей площади земли, доступной в виде лота. Мы также можем рассчитать общее количество площади поверхности дома, TotalSF, сложив площадь от 1-го этажа и 2-го этажа. TotalBath также может быть использован, чтобы сказать нам в общей сложности, сколько ванных комнат есть в доме. Мы также можем добавить все различные типы крыльц вокруг дома и обобщить в общей площади крыльца, TotalPorch.\n",
    "\n",
    "-   TotalLot = LotFrontage + LotArea\n",
    "-   TotalSF = TotalBsmtSF + 2ndFlrSF\n",
    "-   TotalBath = FullBath + HalfBath\n",
    "-   TotalPorch = OpenPorchSF + EnclosedPorch + ScreenPorch\n",
    "-   TotalBsmtFin = BsmtFinSF1 + BsmtFinSF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"TotalLot\"] = train_df[\"LotFrontage\"] + train_df[\"LotArea\"]\n",
    "train_df[\"TotalBsmtFin\"] = train_df[\"BsmtFinSF1\"] + train_df[\"BsmtFinSF2\"]\n",
    "train_df[\"TotalSF\"] = train_df[\"TotalBsmtSF\"] + train_df[\"2ndFlrSF\"]\n",
    "train_df[\"TotalBath\"] = train_df[\"FullBath\"] + train_df[\"HalfBath\"]\n",
    "train_df[\"TotalPorch\"] = (\n",
    "    train_df[\"OpenPorchSF\"] + train_df[\"EnclosedPorch\"] + train_df[\"ScreenPorch\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда очень важно применить к признаку какое-то выпуклое преобразование. Например, логарифм, корень или наоборот возведение в степень.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"LivLotRatio\"] = train_df[\"GrLivArea\"] / train_df[\"LotArea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также включаем создание бинарных столбцов для некоторых признаков, которые могут указывать на наличие(1) / отсутствие(0) некоторых признаков дома\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"MasVnrArea\",\n",
    "    \"TotalBsmtFin\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"TotalPorch\",\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "    col_name = col + \"_bin\"\n",
    "    train_df[col_name] = train_df[col].apply(lambda train_df: 1 if train_df > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, поскольку нам нужны данные, которые носят численный характер, мы преобразуем оставшиеся категориальные столбцы с помощью one-hot-encoding с помощью метода get_dummies() в числовые столбцы, которые подходят для подачи в наш алгоритм машинного обучения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания новых признаков можно использовать обучение без учителя, например, кластеризацию с помощью k средних. Можно использовать как категорию (столбец с 0, 1, 2,...) метки кластеров или расстояние наблюдений до каждого кластера. Эти особенности иногда могут быть эффективными при распутывании сложных пространственных отношений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features = [\n",
    "    \"LotArea\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"GrLivArea\",\n",
    "]\n",
    "\n",
    "\n",
    "def cluster_labels(df, features, n_clusters=20):\n",
    "    X = df.copy()\n",
    "    X_scaled = X.loc[:, features]\n",
    "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n",
    "    X_new = pd.DataFrame()\n",
    "    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def cluster_distance(df, features, n_clusters=20):\n",
    "    X = df.copy()\n",
    "    X_scaled = X.loc[:, features]\n",
    "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n",
    "    X_cd = kmeans.fit_transform(X_scaled)\n",
    "    # Label features and join to dataset\n",
    "    X_cd = pd.DataFrame(X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])])\n",
    "    return X_cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = cluster_labels(train_df, features=cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"cluster_num\"] = cluster_df[\"Cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скейлинг\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть две основные причины, зачем нам скейлинг:\n",
    "\n",
    "1. Многие модели учитывают масштабы для построения прогнозов\n",
    "2. Многие алгоритмы сходятся быстрее для шкалированных данных.\n",
    "\n",
    "Часто используются логарифмирование для данных с сильно несимметричным распределением. Логарифмирование позволяет также решить проблему нелинейности в масштабе. Основная идея в переводе мультипликативной зависимости в аддитивную.\n",
    "Для последнего рассмотрим следующий пример: Для концертов с различным масштабом увеличение аудитории на константу имеет разную значимость. Рассмотрим, что происходит каждый раз, когда размер аудитории увеличивается на 50%. Для начального размера аудитории 100 происходит следующее:\n",
    "$$ \\log*{10}(150) = \\log*{10}(100 \\cdot 1.5) = \\log*{10}(100) + \\log*{10}(1.5) \\approx 2 + 0.176 $$\n",
    "Для аудитории в 1000:\n",
    "$$ \\log*{10}(15,000) = \\log*{10}(10,000 \\cdot 1.5) = \\log*{10}(10,000) + \\log*{10}(1.5) \\approx 4 + 0.176 $$\n",
    "Так мы можем уменьтшить сатурацию важности признака в зависисмости от масштаба. Важно понимать, что мы не пытаемся нормализовать переменную, а именно решить проблему скошенности.\n",
    "\n",
    "**Задание**: Найдите такой признак в наших данных и прологарифмируйте.\n",
    "\n",
    "Что делать, если переменная может быть отрицательной? Для этого можно использовать\n",
    "$$ y = log\\_{base}(x + offset) $$\n",
    "offset позволяет сдвинуть значения в нужную сторону, и лучше он должен быть меньше, чем минимальное возможное. (Если, конечно, оно известно).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"LotArea\"\n",
    "train_df_copy = train_df.copy()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(train_df_copy[feature], bins=50, kde=True)\n",
    "plt.title(feature)\n",
    "\n",
    "train_df_copy[f\"log_{feature}\"] = np.log1p(train_df_copy[feature])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(train_df_copy[f\"log_{feature}\"], bins=50, kde=True)\n",
    "plt.title(f\"log({feature})\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это не универсальное решение. Хотя оно может улучшать распределения с перекосом вправо (где большинство точек данных имеют более низкие значения), он оказывает противоположный эффект на распределение, которое не искажено или смещено влево.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли альтернативы? Итак, нам нужен метод, который преобразует шкалу в неперекошенную, а также работает с отрицательными данными.\n",
    "\n",
    "Квадратный корень может быть тем, что мы ищем. Сам по себе он принимает на вход положительное число и возвращает число, которое при умножении на себя равно входному значению. Это дает желаемый эффект сжатия: большие значения сжимаются сильнее, чем меньшие. Кроме того, поскольку его областью определения являются положительные числа (0 — это особый случай, поскольку он отображается сам в себя), мы можем отразить его, чтобы работать с отрицательными числами так же, как он работал с положительными числами. Это приводит к следующему преобразованию:\n",
    "$$ y = \\text{sign}(x)\\sqrt{\\left| x \\right|} $$\n",
    "\n",
    "У него нет такой возможности сжимать большие значения, как у логарифмов, но зато оно будет легко работать с отрицательными значениями и позволит уловить квадратичные эффекты. Однако, хотя оно сохраняет порядок числовых значений, но не дает нам хорошего способа интерпретировать изменения (и может исказить неискаженные распределения).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скейлинг с обучением\n",
    "\n",
    "RobustScaler - это метод преобразования, который удаляет медиану и масштабирует данные в соответствии с диапазоном квантиля (по умолчанию IQR: межквартильный диапазон). IQR - это диапазон между 1-м квартилем (25-й квантилем) и 3 Квартиль (75-й квантиль). Он также устойчив к пропускам, что делает его идеальным для данных, где слишком много пропусков, что резко сокращает количество обучающих данных.\n",
    "\n",
    "Запуская скейлер как на тренировочном, так и на тестовом наборах, мы подвергаем себя проблеме утечки данных. Утечка данных - это проблема, когда для создания модели используется информация извне набора для обучения. Если мы подгоняем скейлер как на тренировочные, так и на тестовые данные, наши характеристики тренировочных данных будут содержать распределение нашего тестового набора. Таким образом, мы неявно передаем информацию о наших тестовых данных в окончательные тренировочные данные для обучения, что не даст нам возможности по-настоящему протестировать нашу модель на данных, которые она никогда не видела.\n",
    "\n",
    "_Извлеченные уроки:_ Установка скейлера только на обучающие данные, а затем преобразование данных как обучающей, так и тестовой выборок\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "cols = train_df.select_dtypes(np.number).columns\n",
    "# train_df = train_df.drop([\"Id\"], axis=1)\n",
    "transformer = RobustScaler().fit(train_df[cols])\n",
    "train_df[cols] = transformer.transform(train_df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы сильно преобразовали наш обучающий набор. Кроме перчисленного, полезно использовать PCA, выбор признаков на основе информации и други методы. Как вы, наверное, заметили, все преобразования были сделаны только для тренировочного набора, но то же самое необходимо сделать и для тестового.\n",
    "\n",
    "Чтобы предотвратить утечку данных, все преобразования по среднему и тп нужно сделать независимо, а если мы, например, кодировали или удаляли столбцы, нужно сделать такое же преобразование, используя старые правила.\n",
    "\n",
    "После того, как это было сделано, можно передавать данные в модель.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Используя признаки, построенные для обучающего набора, дополните тестовый. Сохраните резщультирующие наборы на будущее.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Id\" in train_df.columns:\n",
    "    train_df = train_df.drop(\"Id\", axis=1)\n",
    "\n",
    "cols = train_df.select_dtypes(include=[np.number]).columns.drop(\n",
    "    \"SalePrice\", errors=\"ignore\"\n",
    ")\n",
    "\n",
    "print(\"src\")\n",
    "print(f\"train_df.shape: {train_df.shape}\")\n",
    "print(f\"test_df.shape: {test_df.shape}\")\n",
    "print(f\"cols: {list(cols)}\")\n",
    "\n",
    "scaler = RobustScaler().fit(train_df[cols])\n",
    "train_df[cols] = scaler.transform(train_df[cols])\n",
    "\n",
    "if \"Id\" in test_df.columns:\n",
    "    test_ids = test_df[\"Id\"]\n",
    "    test_df = test_df.drop(\"Id\", axis=1)\n",
    "\n",
    "test_df = pd.get_dummies(test_df)\n",
    "missing_cols = set(train_df.columns) - set(test_df.columns)\n",
    "\n",
    "for col in missing_cols:\n",
    "    test_df[col] = 0\n",
    "\n",
    "test_df = test_df[train_df.columns.drop(\"SalePrice\", errors=\"ignore\")]\n",
    "test_df[cols] = test_df[cols].fillna(0)\n",
    "test_df[cols] = scaler.transform(test_df[cols])\n",
    "\n",
    "print(\"\\nafter\")\n",
    "print(f\"train_df.shape: {train_df.shape}\")\n",
    "print(f\"test_df.shape: {test_df.shape}\")\n",
    "print(f\"{len(missing_cols)} missing_cols\")\n",
    "\n",
    "if missing_cols:\n",
    "    print(\"cols:\", list(missing_cols))\n",
    "\n",
    "# train_df.to_csv(\"train_preprocessed.csv\", index=False)\n",
    "# test_df.to_csv(\"test_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
